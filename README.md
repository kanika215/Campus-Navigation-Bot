# ğŸ“Œ Campus Navigation Project

Reinforcement Learningâ€“based grid navigation system implemented on a **5Ã—5 campus layout**.  
The agent learns to move from **Hostel (Start)** to **Library (Goal)** while avoiding obstacles.

This project implements and compares four RL algorithms:

- **Q-Learning**
- **SARSA**
- **Monte Carlo Control**
- **Temporal Difference (TD-Î»)**

All code is implemented inside:

ğŸ“„ **Campus Navigation Bot.ipynb**

---

## ğŸ“‘ Table of Contents
- [Project Overview](#project-overview)
- [Features](#features)
- [File Structure](#file-structure)
- [How to Run](#how-to-run)
- [Campus Environment](#campus-environment)
- [Algorithms Implemented](#algorithms-implemented)
  - [Q-Learning](#1ï¸âƒ£-q-learning)
  - [SARSA](#2ï¸âƒ£-sarsa)
  - [Monte Carlo Control](#3ï¸âƒ£-monte-carlo-control)
  - [Temporal Difference (TD-Î»)](#4ï¸âƒ£-temporal-difference-td-Î»)
- [Output Visualizations](#output-visualizations)
- [Use Cases](#use-cases)
- [Requirements](#requirements)


---

## ğŸ« **Project Overview**
This project simulates a **campus navigation environment** represented as a 5Ã—5 grid.  
Each grid cell corresponds to a real-world campus location such as:

- Hostel (Start)
- Walkways
- Admin Buildings (Obstacle)
- Park (Obstacle)
- Sports Complex (Obstacle)
- Library (Goal)

The goal of the agent is to learn the **optimal path** using different Reinforcement Learning algorithms.

---

## âœ¨ **Features**
- âœ” Custom 5Ã—5 campus grid  
- âœ” Four RL algorithms  
- âœ” Arrow-based navigation path visualization  
- âœ” Reward vs Episode learning curves  
- âœ” Q-Table generation  
- âœ” Same environment for all algorithms  
- âœ” Clean, modular code  

---

## ğŸ“ **File Structure**

```
Campus Navigation Bot.ipynb  
README.md
```

Notebook includes:

- Environment setup  
- Rewards  
- Step function  
- Visualization functions  
- Q-Learning / SARSA  
- Monte Carlo Control  
- TD-Î»  
- Final outputs  

---

## â–¶ï¸ **How to Run**

### **1ï¸âƒ£ Install Dependencies**
```bash
pip install numpy matplotlib
```

### **2ï¸âƒ£ Run the Notebook**
Open `Campus Navigation Bot.ipynb` in:

- Jupyter Notebook  
- OR VS Code (with Jupyter extension)

Run all cells to see outputs.

---

## ğŸ—º **Campus Environment**

The campus is a 5Ã—5 grid with rewards:

| Location | Reward |
|---------|--------|
| Walkway | -1 |
| Admin / Park / Sports Complex (Obstacles) | -5 |
| Goal (Library) | +10 |
| Invalid Move | Penalty |

Obstacles block movement.

---

# ğŸ” **Algorithms Implemented**

## 1ï¸âƒ£ Q-Learning
**Off-policy TD control**  
Update rule:
```
Q[s][a] = Q[s][a] + Î± * (r + Î³ * max(Q[s']) - Q[s][a])
```

Outputs:
- Q-table  
- Optimal path  
- Reward curve  

---

## 2ï¸âƒ£ SARSA
**On-policy TD control**  
Update rule:
```
Q[s][a] = Q[s][a] + Î± * (r + Î³ * Q[sâ€™][aâ€™] - Q[s][a])
```

Outputs:
- Q-table  
- Optimal path  
- Reward graph  

---

## 3ï¸âƒ£ Monte Carlo Control
Episode-based learning.  
Updates occur after each episode.

Outputs:
- Path  
- Costs  
- Reward curves  

---

## 4ï¸âƒ£ Temporal Difference (TD-Î»)
Uses **eligibility traces**, faster convergence.

Outputs:
- Path  
- Costs  
- Learning curve  

---

## ğŸ“Š **Output Visualizations**
The notebook produces:

- ğŸ“ˆ Reward vs Episodes  
- ğŸ§­ Arrow-based optimal path  
- ğŸŸ© Campus grid  
- ğŸ§® Q-Tables  
- ğŸ”„ Algorithm comparisons  

---

## ğŸ§  **Use Cases**
Useful for:

- RL learning  
- Gridworld experiments  
- University projects  
- Navigation simulations  
- Algorithm comparison studies  

---

## ğŸ›  **Requirements**
- Python 3.8+  
- numpy  
- matplotlib  

---

If you want a **project banner**, **badges**, or **GIF of path animation**, tell me! ğŸš€
