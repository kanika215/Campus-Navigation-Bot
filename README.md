# üéì Campus Navigation Project

Reinforcement Learning‚Äìbased grid navigation system implemented on a 5√ó5 campus layout.  
The agent learns to move from Hostel (Start) to Library (Goal) while avoiding obstacles.

This project implements and compares four RL algorithms:

- Q-Learning  
- SARSA  
- Monte Carlo Control  
- Temporal Difference (TD-Œª)

All code is implemented in:

üìÑ **Campus Navigation Bot.ipynb**

---

## üìë Table of Contents
- [Project Overview](#project-overview)
- [Features](#features)
- [File Structure](#file-structure)
- [How to Run](#how-to-run)
- [Campus Environment](#campus-environment)
- [Algorithms Implemented](#algorithms-implemented)
  - [Q-Learning](#q-learning)
  - [SARSA](#sarsa)
  - [Monte Carlo Control](#monte-carlo-control)
  - [Temporal Difference (TD-Œª)](#temporal-difference-td-Œª)
- [Output Visualizations](#output-visualizations)
- [Use Cases](#use-cases)
- [Requirements](#requirements)

---

## Project Overview

The environment is a 5√ó5 grid representing a campus map.  
Each cell corresponds to a location such as:

- Hostel  
- Walkways  
- Admin Building (Obstacle)  
- Park (Obstacle)  
- Sports Complex (Obstacle)  
- Library (Goal)

The agent receives rewards based on movement, collisions, and reaching the goal.

This project showcases how four RL algorithms learn optimal navigation paths on the **same campus layout**.

---

## Features

‚úî Custom 5√ó5 campus environment  
‚úî Four RL algorithms implemented from scratch  
‚úî Arrow-based path visualization  
‚úî Campus grid with labeled buildings  
‚úî Reward trend graphs  
‚úî Q-table outputs (Q-Learning & SARSA)  
‚úî Full comparison of algorithm behavior

---

## File Structure

```
Campus Navigation Bot.ipynb   ‚Üí Complete implementation
README.md                     ‚Üí Project documentation
```

Contents inside the notebook:

- Environment setup (grid, labels, obstacles, movement rewards)
- Q-Learning  
- SARSA  
- Monte Carlo Control  
- Temporal Difference (TD-Œª)  
- Visualization utilities

---

## How to Run

### Step 1: Install dependencies
```bash
pip install numpy matplotlib
```

### Step 2: Execute the notebook
Run:

```
Campus Navigation Bot.ipynb
```

The notebook will:

- Train all RL algorithms
- Display reward graphs
- Visualize optimal navigation paths
- Print Q-tables (Q-Learning, SARSA)

---

## Campus Environment

Reward structure:

| Location Type | Reward |
|--------------|--------|
| Walkway | -1 |
| Obstacles (Admin, Park, Sports) | -5 |
| Goal (Library) | +10 |
| Invalid Move | Penalty |

The grid is fully labeled and visualized.

---

## Algorithms Implemented

### Q-Learning
Off-policy TD control method  
Update rule:

```
Q[s][a] = Q[s][a] + Œ± * (r + Œ≥ * max(Q[s‚Äô]) - Q[s][a])
```

Outputs:
- Q-Table  
- Total cost  
- Optimal path  
- Reward graph  

---

### SARSA
On-policy TD control method  
Update rule:

```
Q[s][a] = Q[s][a] + Œ± * (r + Œ≥ * Q[s‚Äô][a‚Äô] - Q[s][a])
```

Outputs:
- Q-Table  
- Total cost  
- Optimal path  
- Reward graph  

---

### Monte Carlo Control

Learns action-values using full-episode returns.  
No bootstrapping.

Outputs:
- Optimal path  
- Total cost  
- Reward graph

---

### Temporal Difference (TD-Œª)

Combines TD learning with eligibility traces.  
More efficient propagation of reward information.

Outputs:
- Optimal path  
- Total cost  
- Reward graph

---

## Output Visualizations

The project automatically generates:

- Reward curves for all algorithms  
- Grid-based path visualizations  
- Arrow-based movement paths  
- Q-tables  
- Final optimal policies  

---

## Use Cases

This project is ideal for:

- RL learning and comparison  
- University projects  
- Research demonstrations  
- Understanding convergence behavior  
- Pathfinding and navigation systems  

---

## Requirements

- Python 3.8+
- numpy
- matplotlib

---

## ‚≠ê Author
Developed by Kanika

